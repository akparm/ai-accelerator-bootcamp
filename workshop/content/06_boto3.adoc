# Boto3 exploration

## In this module you will use boto3 to explore the existing minio configuration and set up scripts to automate uploading the models to the locations you need for your pipelines to run. 

We open your workbench and create a new notebook.   

## Steps to create  

* Create a script to explore the minio deployment. 

[source,py]
#!/usr/bin/env python
# coding: utf-8

# Boto3 interactive exploration
import boto3
from botocore.client import Config

# Configuration
minio_url = "https://minio-api-ai-example-training.apps.cluster-5nrzb.dynamic.redhatworkshops.io"
access_key = "minio"
secret_key = "minio123"

# Setting up the MinIO client
s3 = boto3.client(
    's3',
    endpoint_url=minio_url,
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    config=Config(signature_version='s3v4'),
)

# Function to get MinIO server info
def get_minio_info():
    # This function retrieves the list of buckets as an example. 
    # MinIO admin info is not directly supported by boto3; you'd need to use MinIO's admin API.
    response = s3.list_buckets()
    print("Buckets:")
    for bucket in response['Buckets']:
        print(f'  {bucket["Name"]}')

# Setting alias (not applicable in Boto3, but setup is similar to configuring the client)
print("MinIO client configured successfully.")

# Getting MinIO server info
get_minio_info()

* Create a script to upload the model to your s3 pipeline bucket

[source,python]

#!/usr/bin/env python
# coding: utf-8

import boto3
from botocore.client import Config

# Configuration
minio_url = "https://minio-api-ai-example-training.apps.cluster-5nrzb.dynamic.redhatworkshops.io"
access_key = "minio"
secret_key = "minio123"
bucket_name = "pipelines"
file_path = "accident_detect.onnx"
object_name = "accident_detect.onnx"  # You can change this if you want a different object name

# Setting up the MinIO client
s3 = boto3.client(
    's3',
    endpoint_url=minio_url,
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    config=Config(signature_version='s3v4'),
)

# Function to upload a file to a bucket
def upload_file(file_path, bucket_name, object_name):
    try:
        s3.upload_file(file_path, bucket_name, object_name)
        print(f"File '{file_path}' successfully uploaded to bucket '{bucket_name}' as '{object_name}'.")
    except Exception as e:
        print(f"Error uploading file '{file_path}' to bucket '{bucket_name}': {e}")

# Upload the file
upload_file(file_path, bucket_name, object_name)








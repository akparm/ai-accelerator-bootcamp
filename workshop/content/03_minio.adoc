# bootstrap.sh

## The bootstrap.sh run 

The bootstrap script will perform the following steps. 

1. install the gitops
2. create Argo namespace
3. start overlay deployment chosen from the bootstrap_cluster prompt

### Current options:
* rhoai-eus-2.8 
* rhoai-eus-2.8-aws-gpu 
* rhoai-fast 
* rhoai-fast-aws-gpu

The minio resources should be created for you by the tenant ApplicationSet. 

You should find your minio resource in the ai-example-training namespace.
 
include::https://raw.githubusercontent.com/redhat-ai-services/ai-accelerator/main/documentation/training_and_learning/data_science_pipeline/README.md[]

### Create and Run a RHOAI Data Science Pipeline

In order to get RHOAI Data Science Pipelines to work, we need to have OpenShift Pipelines and RHOAI operators installed.
We also need an S3 storage bucket. In this tutorial, the operators should be already be installed.

### Setting up and using Minio.
 1. Create `minio` namespace or use whatever namespace you would like.
 2. Change to new namespace
 3. Apply the _minio.yaml_ file.
    This will create the PVC, admin password secret, deployment, service, api route, and UI route.
 4. The credentials to log into minio are from the secret `minio-secret` from the minio.yaml to login. `minio:minio123`
 5. Create new Data Science Project or apply the `ds-sample-project-ns.yaml`. This will create a new Data Science Project named: `datascience-sample-project`
 6. In the RHOAI Dashboard, configure a new pipeline server. Or you can apply the `pipeline-server.yaml`. (make sure the namespace is correct.)
    Enter the S3 information. Access key is `minio`. Secret key is `minio123`. Endpoint is the minio API endpoint. Bucket: you can create a new bucket on the `minio-ui` or if left blank, it will create a new bucket in S3.
 7. Import a new pipeline. Example: `cointoss.yaml`
 8. After importing a new pipeline, press the 3 kebab menu and create run. Give the run a name and create.
 9. The pipeline will now run. You can go into OpenShift Dashboard>Pipelines>Pipeline Runs to see more details about the pipeline run.

# Working with Pipelines

In this module we will be working with pipelines. We will be creating a pipeline that will train a model and then deploy it to a serving runtime.

## Creating a pipeline using gitops.

We will be using tekton pipelines to automate the deployment of a model-retraining job. To create a pipeline, we will need to create a pipeline definition file. We will create a couple of folders in our parasol-insuance tenent. .

* Navigate to the cloned folder with the command:
[source,terminal]
----
cd ai-accelerator/
----

. Copy `tenants/ai-example/dsp-example-pipeline` and paste it as `tenants/parasol-insurance/dsp-example`.

. Change `line 17` of `tenants/parasol-insurance/dsp-example-pipeline/base/dsp-example-pipeline.yaml`

     - default: 'image-registry.openshift-image-registry.svc:5000/parasol-insurance/dsp-example'

. Change `line 20` of `tenants/parasol-insurance/dsp-example-pipeline/base/dsp-example-pipeline.yaml`

     - default: 'https://ds-pipeline-dspa.parasol-insurance.svc.cluster.local:8443'

. Change `line 4` of  `tenants/parasol-insurance/dsp-example-pipeline/base/kustomization.yaml`

 namespace: parasol-insurance

. Copy `tenants/ai-example/datascience-pipelines` and paste it as `tenants/parasol-insurance/datascience-pipelines`.

. Change `line 33` of tenants/parasol-insurance/datascience-pipelines/base/dspa.yaml

       host: 'minio.parasol-insurance.svc.cluster.local:9000'

. Change `line 4` of tenants/parasol-insurance/datascience-pipelines/base/kustomization.yaml 

 namespace: parasol-insurance








## Diego script modifications.  
. Create a python script to train the model. This script will be used in the pipeline to train the model. Save the script as `train_model.py`:

[source,python]
----
import os
import requests
import zipfile
import boto3

from tqdm.notebook import tqdm
from ultralytics import YOLO
from botocore.client import Config

# Load model
model = YOLO('yolov8m.pt')  # load a pretrained model (recommended for training)

# S3 Configuration
minio_url = os.environ["AWS_S3_ENDPOINT"]
access_key = os.environ["AWS_ACCESS_KEY_ID"]
secret_key = os.environ["AWS_SECRET_ACCESS_KEY"]

# Setting up the MinIO client
s3 = boto3.client(
    's3',
    endpoint_url=minio_url,
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    config=Config(signature_version='s3v4'),
)

# Function to retrieve a specific dataset
def retrieve_dataset(dataset_type):

    # Check if the directory exists, if not, create it
    if not os.path.exists("./datasets/"):
        os.makedirs("./datasets/")

    URL = f"https://rhods-public.s3.amazonaws.com/sample-data/accident-data/accident-{dataset_type}.zip"

    # Check if the file exists, if not, download and unzip it
    if not os.path.exists(f"./datasets/accident-{dataset_type}.zip"):
        print("Downloading file...")
        response = requests.get(URL, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        block_size = 1024
        t = tqdm(total=total_size, unit='iB', unit_scale=True)
        with open(f'./datasets/accident-{dataset_type}.zip', 'wb') as f:
            for data in response.iter_content(block_size):
                t.update(len(data))
                f.write(data)
        t.close()
    if os.path.exists(f"./datasets/accident-{dataset_type}.zip"):
        print("Unzipping file...")
        with zipfile.ZipFile(f'./datasets/accident-{dataset_type}.zip', 'r') as zip_ref:
            zip_ref.extractall(path='./datasets/')
    print("Done!")
  
# Function to upload a file to a bucket
def upload_file(file_path, bucket_name, object_name):
    try:
        s3.upload_file(file_path, bucket_name, object_name)
        print(f"File '{file_path}' successfully uploaded to bucket '{bucket_name}' as '{object_name}'.")
    except Exception as e:
        print(f"Error uploading file '{file_path}' to bucket '{bucket_name}': {e}")


dataset_type = 'sample'
# dataset_type = 'full' # Use this line instead if you want to retrieve the full dataset
retrieve_dataset(dataset_type)

# Train the model
results = model.train(data='./datasets/accident-sample/data.yaml', epochs=1, imgsz=640, batch=2)

ObjDetOXModel = YOLO("runs/detect/train/weights/best.pt").export(format="onnx")

# Use the current timestamp as the model version in the file name
timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
model_filename = f"model_{timestamp}.onnx"

# Rename the onnx model, it is currently at runs/detect/train/weights/best.onnx
os.rename("runs/detect/train/weights/best.onnx", model_filename)

# upload the model s3
upload_file(f"runs/detect/train/weights/{model_filename}", "models", f"accident_model/{model_filename}")

----

. Create a python script to store the model in the S3 bucket. This script will be used in the pipeline to store the model in the S3 bucket. Save the script as `store_model.py`:

[source,python]
----
import boto3

# Load the model
model = joblib.load('model.joblib')

# Store the model in the S3 bucket
s3 = boto3.client('s3')
s3.upload_file('model.joblib', 'parasol-insurance', 'model.joblib')
----

. Create a python script to commit and push a change to the GitOps repository. This script will be used in the pipeline to commit and push a change to the GitOps repository. Save the script as `commit_and_push.py`:

[source,python]
----
import os

# Commit and push the change to the GitOps repository
os.system('git add .')
os.system('git commit -m "Add model.joblib"')
os.system('git push')
----



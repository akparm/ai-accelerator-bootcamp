# Working with Pipelines

In this module we will be working with pipelines. We will be creating a pipeline that will train a model and then deploy it to a serving runtime.

## Creating a pipeline using gitops.

We will be using pipelines to automate the deployment of a model-retraining job. To create a pipeline, we will need to create a pipeline definition file. We will create a couple of folders in our parasol-insuance tenent.

. Navigate to the cloned folder with the command:

+
[source,terminal]
----
cd ai-accelerator/
----

. Copy `tenants/ai-example/datascience-pipelines` and paste it as `tenants/parasol-insurance/datascience-pipelines`.

. Modify file `tenants/parasol-insurance/datascience-pipelines/base/dspa.yaml`.

+
[source,yaml]
----
apiVersion: datasciencepipelinesapplications.opendatahub.io/v1alpha1
kind: DataSciencePipelinesApplication
metadata:
  name: dspa
spec:
  apiServer:
    stripEOF: true
    dbConfigConMaxLifetimeSec: 120
    applyTektonCustomResource: true
    deploy: true
    enableSamplePipeline: false
    autoUpdatePipelineDefaultVersion: true
    archiveLogs: false
    terminateStatus: Cancelled
    enableOauth: true
    trackArtifacts: true
    collectMetrics: true
    injectDefaultScript: true
  database:
    disableHealthCheck: false
    mariaDB:
      deploy: true
      pipelineDBName: mlpipeline
      pvcSize: 10Gi
      username: mlpipeline
  dspVersion: v2
  objectStorage:
    disableHealthCheck: false
    enableExternalRoute: false
    externalStorage:
      basePath: ''
      bucket: pipelines
      host: 'minio.parasol-insurance.svc.cluster.local:9000'
      port: ''
      region: us-east-1
      s3CredentialsSecret:
        accessKey: minio_root_user
        secretKey: minio_root_password
        secretName: minio
      scheme: http
  persistenceAgent:
    deploy: true
    numWorkers: 2
  scheduledWorkflow:
    cronScheduleTimezone: UTC
    deploy: true
----

. Modify file `tenants/parasol-insurance/datascience-pipelines/base/kustomization.yaml`.

+
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - ../../../../components/apps/minio/overlays/default
  - dspa.yaml
----

. Copy `tenants/ai-example/dsp-example-pipeline` and paste it as `tenants/parasol-insurance/model-training-pipeline`.

. Modify the following file `tenants/parasol-insurance/model-training-pipeline/base/model-training-pipeline.yaml`.

+
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: dsp-example
spec:
  params:
    - default: 'https://github.com/redhat-ai-services/ai-accelerator-bootcamp.git'
      description: Repo URL
      name: GIT_URL
      type: string
    - default: 'source_code/40_pipelines'
      description: Repo URL
      name: GIT_CONTEXT
      type: string
    - default: 'train-car-rekon.py'
      name: PIPELINE_SCRIPT
      type: string
    - default: main
      name: GIT_REVISION
      type: string
    - default: 3.11-ubi9
      name: PYTHON_IMAGE
      type: string
    - default: 'image-registry.openshift-image-registry.svc:5000/parasol-insurance/dsp-example'
      name: TARGET_IMAGE
      type: string
    - default: 'https://ds-pipeline-dspa.parasol-insurance.svc.cluster.local:8443'
      name: KUBEFLOW_ENDPOINT
      type: string
  tasks:
    - name: git-clone
      params:
        - name: url
          value: $(params.GIT_URL)
        - name: revision
          value: $(params.GIT_REVISION)
        - name: gitInitImage
          value: 'registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8@sha256:868966ef9d4b54952d8a74eb83bba40eb1f52c0148994fa704efd0e3797c61c5'
      taskRef:
        kind: ClusterTask
        name: git-clone
      workspaces:
        - name: output
          workspace: source
    - name: s2i-python
      params:
        - name: VERSION
          value: $(params.PYTHON_IMAGE)
        - name: PATH_CONTEXT
          value: $(params.GIT_CONTEXT)
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
      runAfter:
        - git-clone
      taskRef:
        kind: ClusterTask
        name: s2i-python
      workspaces:
        - name: source
          workspace: source
    - name: execute-kubeflow-pipeline
      params:
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
        - name: TAG
          value: latest
        - name: SCRIPT
          value: $(params.PIPELINE_SCRIPT)
        - name: KUBEFLOW_ENDPOINT
          value: $(params.KUBEFLOW_ENDPOINT)
      runAfter:
        - s2i-python
      taskRef:
        kind: Task
        name: execute-kubeflow-pipeline
  workspaces:
    - name: source
----

. Modify file `tenants/parasol-insurance/model-training-pipeline/base/execute-kfp-task.yaml`.

+
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: execute-kubeflow-pipeline
spec:
  description: >-
    This task will execute a python script, triggering a kubeflow pipeline
  params:
    - name: IMAGE
      description: The image used to execute the script
      type: string
    - name: TAG
      description: The tag for the image
      type: string
      default: "latest"
    - name: SCRIPT
      description: The location of the script to be executed
    - name: KUBEFLOW_ENDPOINT
      description: The endpoint URL for Kubeflow
      default: "https://ds-pipeline-dspa:8443"
  steps:
    - name: execute-python
      image: $(inputs.params.IMAGE):$(inputs.params.TAG)
      env:
        - name: KUBEFLOW_ENDPOINT
          value: $(inputs.params.KUBEFLOW_ENDPOINT)
      script: |
        set -x
        echo "lets start this cript"
        pwd
        ls -la
        echo "about to execute python now"
        python $(inputs.params.SCRIPT)
        set +x
----

. Modify file `tenants/parasol-insurance/model-training-pipeline/base/kustomization.yaml`

+
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - dsp-example-imagestream.yaml
  - dsp-example-pipeline.yaml
  - dsp-example-pipelinerun.yaml
  - dsp-example-rbac.yaml
  - execute-kfp-task.yaml
----

## Diego script modifications.  
1. Create a python script to train the model. This script will be used in the pipeline to train the model. Save the script as `train_model.py`:

[source,python]
----
import os
import requests
import zipfile
import boto3

from tqdm.notebook import tqdm
from ultralytics import YOLO
from botocore.client import Config

# Load model
model = YOLO('yolov8m.pt')  # load a pretrained model (recommended for training)

# S3 Configuration
minio_url = os.environ["AWS_S3_ENDPOINT"]
access_key = os.environ["AWS_ACCESS_KEY_ID"]
secret_key = os.environ["AWS_SECRET_ACCESS_KEY"]

# Setting up the MinIO client
s3 = boto3.client(
    's3',
    endpoint_url=minio_url,
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    config=Config(signature_version='s3v4'),
)

# Function to retrieve a specific dataset
def retrieve_dataset(dataset_type):

    # Check if the directory exists, if not, create it
    if not os.path.exists("./datasets/"):
        os.makedirs("./datasets/")

    URL = f"https://rhods-public.s3.amazonaws.com/sample-data/accident-data/accident-{dataset_type}.zip"

    # Check if the file exists, if not, download and unzip it
    if not os.path.exists(f"./datasets/accident-{dataset_type}.zip"):
        print("Downloading file...")
        response = requests.get(URL, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        block_size = 1024
        t = tqdm(total=total_size, unit='iB', unit_scale=True)
        with open(f'./datasets/accident-{dataset_type}.zip', 'wb') as f:
            for data in response.iter_content(block_size):
                t.update(len(data))
                f.write(data)
        t.close()
    if os.path.exists(f"./datasets/accident-{dataset_type}.zip"):
        print("Unzipping file...")
        with zipfile.ZipFile(f'./datasets/accident-{dataset_type}.zip', 'r') as zip_ref:
            zip_ref.extractall(path='./datasets/')
    print("Done!")
  
# Function to upload a file to a bucket
def upload_file(file_path, bucket_name, object_name):
    try:
        s3.upload_file(file_path, bucket_name, object_name)
        print(f"File '{file_path}' successfully uploaded to bucket '{bucket_name}' as '{object_name}'.")
    except Exception as e:
        print(f"Error uploading file '{file_path}' to bucket '{bucket_name}': {e}")


dataset_type = 'sample'
# dataset_type = 'full' # Use this line instead if you want to retrieve the full dataset
retrieve_dataset(dataset_type)

# Train the model
results = model.train(data='./datasets/accident-sample/data.yaml', epochs=1, imgsz=640, batch=2)

ObjDetOXModel = YOLO("runs/detect/train/weights/best.pt").export(format="onnx")

# Use the current timestamp as the model version in the file name
timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
model_filename = f"model_{timestamp}.onnx"

# Rename the onnx model, it is currently at runs/detect/train/weights/best.onnx
os.rename("runs/detect/train/weights/best.onnx", model_filename)

# upload the model s3
upload_file(f"runs/detect/train/weights/{model_filename}", "models", f"accident_model/{model_filename}")

----

[start=2]
2. Create a python script to store the model in the S3 bucket. This script will be used in the pipeline to store the model in the S3 bucket. Save the script as `store_model.py`:

[source,python]
----
import boto3

# Load the model
model = joblib.load('model.joblib')

# Store the model in the S3 bucket
s3 = boto3.client('s3')
s3.upload_file('model.joblib', 'parasol-insurance', 'model.joblib')
----

[start=3]
3. Create a python script to commit and push a change to the GitOps repository. This script will be used in the pipeline to commit and push a change to the GitOps repository. Save the script as `commit_and_push.py`:

[source,python]
----
import os

# Commit and push the change to the GitOps repository
os.system('git add .')
os.system('git commit -m "Add model.joblib"')
os.system('git push')
----


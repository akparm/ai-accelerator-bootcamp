# Working with Pipelines

In this module we will be working with pipelines. We will be creating a pipeline that will train a model and then deploy it to a serving runtime.

## Creating a pipeline using gitops.

. Create a `data-science-pipelines` directory in the `parasol-insurance` tenand directory.

. Create the `base` and `overlays` directories in the `data-science-pipelines` directory.

. In the `base` directory, create a `kustomization.yaml` file with the following content:

+
.kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - dspa.yaml
----

. In the `base` directory, create a `dspa.yaml` file with the following content:

+
.dspa.yaml
[source,yaml]
----
apiVersion: datasciencepipelinesapplications.opendatahub.io/v1alpha1
kind: DataSciencePipelinesApplication
metadata:
  name: dspa
spec:
  apiServer:
    stripEOF: true
    dbConfigConMaxLifetimeSec: 120
    applyTektonCustomResource: true
    deploy: true
    enableSamplePipeline: false
    autoUpdatePipelineDefaultVersion: true
    archiveLogs: false
    terminateStatus: Cancelled
    enableOauth: true
    trackArtifacts: true
    collectMetrics: true
    injectDefaultScript: true
  database:
    disableHealthCheck: false
    mariaDB:
      deploy: true
      pipelineDBName: mlpipeline
      pvcSize: 10Gi
      username: mlpipeline
  dspVersion: v2
  objectStorage:
    disableHealthCheck: false
    enableExternalRoute: false
    externalStorage:
      basePath: ''
      bucket: pipelines
      host: 'minio.object-datastore.svc.cluster.local:9000'
      port: ''
      region: us-east-1
      s3CredentialsSecret:
        accessKey: AWS_ACCESS_KEY_ID
        secretKey: AWS_SECRET_ACCESS_KEY
        secretName: accident-model-data-conn
      scheme: http
  persistenceAgent:
    deploy: true
    numWorkers: 2
  scheduledWorkflow:
    cronScheduleTimezone: UTC
    deploy: true
----

+
[TIP]
====
Your Data Science Cluster requires of a dspa, so that you can create pipelines.

Before creating the dspa object, your Data Science Project shows a button to *Configure pipeline server*:

image::images/41_working_with_pipelines/01-configure-pipeline-server.png[Configure pipeline server]

Instead of creating the pipeline server configuration using the RHOAI method that you have seen in previous enablemens, in these steps you are using the GitOps method to create the pipeline server configuration.

====

. In the `overlays` directory, create a `kustomization.yaml` file with the following content:

+
.kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../base
----


. Copy `tenants/ai-example/datascience-pipelines` and paste it as `tenants/parasol-insurance/datascience-pipelines`.

. Modify file `tenants/parasol-insurance/datascience-pipelines/base/dspa.yaml`.

+
[source,yaml]
----
apiVersion: datasciencepipelinesapplications.opendatahub.io/v1alpha1
kind: DataSciencePipelinesApplication
metadata:
  name: dspa
spec:
  apiServer:
    stripEOF: true
    dbConfigConMaxLifetimeSec: 120
    applyTektonCustomResource: true
    deploy: true
    enableSamplePipeline: false
    autoUpdatePipelineDefaultVersion: true
    archiveLogs: false
    terminateStatus: Cancelled
    enableOauth: true
    trackArtifacts: true
    collectMetrics: true
    injectDefaultScript: true
  database:
    disableHealthCheck: false
    mariaDB:
      deploy: true
      pipelineDBName: mlpipeline
      pvcSize: 10Gi
      username: mlpipeline
  dspVersion: v2
  objectStorage:
    disableHealthCheck: false
    enableExternalRoute: false
    externalStorage:
      basePath: ''
      bucket: pipelines
      host: 'minio.object-datastore.svc.cluster.local:9000'
      port: ''
      region: us-east-1
      s3CredentialsSecret:
        accessKey: AWS_ACCESS_KEY_ID
        secretKey: AWS_SECRET_ACCESS_KEY
        secretName: accident-model-data-conn
      scheme: http
  persistenceAgent:
    deploy: true
    numWorkers: 2
  scheduledWorkflow:
    cronScheduleTimezone: UTC
    deploy: true
----

. Modify file `tenants/parasol-insurance/datascience-pipelines/base/kustomization.yaml`.

+
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - dspa.yaml
----

. Copy `tenants/ai-example/dsp-example-pipeline` and paste it as `tenants/parasol-insurance/model-training-pipeline`.

. Modify the following file `tenants/parasol-insurance/model-training-pipeline/base/model-training-pipeline.yaml`.

+
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: dsp-example
spec:
  params:
    - default: 'https://github.com/redhat-ai-services/ai-accelerator-bootcamp.git'
      description: Repo URL
      name: GIT_URL
      type: string
    - default: 'source_code/40_pipelines'
      description: Repo URL
      name: GIT_CONTEXT
      type: string
    - default: 'train-car-rekon.py'
      name: PIPELINE_SCRIPT
      type: string
    - default: main
      name: GIT_REVISION
      type: string
    - default: 3.11-ubi9
      name: PYTHON_IMAGE
      type: string
    - default: 'image-registry.openshift-image-registry.svc:5000/parasol-insurance/dsp-example'
      name: TARGET_IMAGE
      type: string
    - default: 'https://ds-pipeline-dspa.parasol-insurance.svc.cluster.local:8443'
      name: KUBEFLOW_ENDPOINT
      type: string
  tasks:
    - name: git-clone
      params:
        - name: url
          value: $(params.GIT_URL)
        - name: revision
          value: $(params.GIT_REVISION)
        - name: gitInitImage
          value: 'registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8@sha256:868966ef9d4b54952d8a74eb83bba40eb1f52c0148994fa704efd0e3797c61c5'
      taskRef:
        kind: ClusterTask
        name: git-clone
      workspaces:
        - name: output
          workspace: source
    - name: s2i-python
      params:
        - name: VERSION
          value: $(params.PYTHON_IMAGE)
        - name: PATH_CONTEXT
          value: $(params.GIT_CONTEXT)
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
      runAfter:
        - git-clone
      taskRef:
        kind: ClusterTask
        name: s2i-python
      workspaces:
        - name: source
          workspace: source
    - name: execute-kubeflow-pipeline
      params:
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
        - name: TAG
          value: latest
        - name: SCRIPT
          value: $(params.PIPELINE_SCRIPT)
        - name: KUBEFLOW_ENDPOINT
          value: $(params.KUBEFLOW_ENDPOINT)
      runAfter:
        - s2i-python
      taskRef:
        kind: Task
        name: execute-kubeflow-pipeline
  workspaces:
    - name: source
----

. Modify file `tenants/parasol-insurance/model-training-pipeline/base/execute-kfp-task.yaml`.

+
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: execute-kubeflow-pipeline
spec:
  description: >-
    This task will execute a python script, triggering a kubeflow pipeline
  params:
    - name: IMAGE
      description: The image used to execute the script
      type: string
    - name: TAG
      description: The tag for the image
      type: string
      default: "latest"
    - name: SCRIPT
      description: The location of the script to be executed
    - name: KUBEFLOW_ENDPOINT
      description: The endpoint URL for Kubeflow
      default: "https://ds-pipeline-dspa:8443"
  steps:
    - name: execute-python
      image: $(inputs.params.IMAGE):$(inputs.params.TAG)
      env:
        - name: KUBEFLOW_ENDPOINT
          value: $(inputs.params.KUBEFLOW_ENDPOINT)
      script: |
        set -x
        echo "lets start this cript"
        pwd
        ls -la
        echo "about to execute python now"
        python $(inputs.params.SCRIPT)
        set +x
----

. Modify file `tenants/parasol-insurance/model-training-pipeline/base/kustomization.yaml`

+
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - dsp-example-imagestream.yaml
  - dsp-example-pipeline.yaml
  - dsp-example-pipelinerun.yaml
  - dsp-example-rbac.yaml
  - execute-kfp-task.yaml
----


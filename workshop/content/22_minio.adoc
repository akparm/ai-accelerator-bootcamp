# Minio S3 Storage
The minio resources should be created for you by the tenant ApplicationSet when we installed GitOps and ran the bootstrap script. 

._Set up minio manually (if needed)_
[%collapsible]
====
 1. Create `minio` namespace or use whatever namespace you would like
 2. Change to new namespace and change to new namespace.
 3. Apply the _**pvc.yaml**_, _**secret.yaml**_, _**service.yaml**_, _**routes.yaml**_, and _**deployment.yaml**_ from the https://github.com/redhat-ai-services/ai-accelerator/tree/main/components/apps/minio/base[ai-accelerator project].
    This will create the PVC, admin password secret, deployment, service, api route, and UI route.
 4. The credentials to log into minio are from the secret `minio-secret` from the secret.yaml to login. `minio:minio123`
====
[%collapsible]

You should find your minio resource in the _**ai-example-training**_ namespace.

The *minio-ui* route can be found in _**ai-example-training**_ namespace under _**Routes**_. Open this in a new tab and log in with `minio:minio123`.
Explore the S3 storage.
Create buckets and add a files and folder to it.

### Explore S3 in Workbench:
Sometimes you won't have access to a customer's S3 storage UI. If that is the case, we can explore an S3 storage using a workbench and python code.

1. Go to RHOAI Dashboard and go to the _**ai-example-training**_ Data Science Project.

image::images/DSP_ai-example-training.png[width:50%]

2. As you can see there is a workbench running named _example-workbench_. This workbench

 5. Create new Data Science Project or apply the `ds-sample-project-ns.yaml`. This will create a new Data Science Project named: `datascience-sample-project`
 6. In the RHOAI Dashboard, configure a new pipeline server. Or you can apply the `pipeline-server.yaml`. (make sure the namespace is correct.)
    Enter the S3 information. Access key is `minio`. Secret key is `minio123`. Endpoint is the minio API endpoint. Bucket: you can create a new bucket on the `minio-ui` or if left blank, it will create a new bucket in S3.


 7. Import a new pipeline. Example: `/source/code/22_minio/compiled_pipeline.yaml`
 8. After importing a new pipeline, press the 3 kebab menu and create run. Give the run a name and create.
 9. The pipeline will now run. You can go into OpenShift Dashboard>Pipelines>Pipeline Runs to see more details about the pipeline run.

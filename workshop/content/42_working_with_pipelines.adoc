# Working with Pipelines

In this module we will be working with pipelines. We will be creating a pipeline that will train a model and then deploy it to a serving runtime.

## Develop a KFP pipeline to retrain a model

In the parasol-insurance repository, there is a python notebook that re-trains a model: https://github.com/rh-aiservices-bu/parasol-insurance/blob/dev/lab-materials/04/04-03-model-retraining.ipynb[04-03-model-retraining.ipynb]. We will be using this notebook to create a pipeline that will retrain the model.

. In RHOAI, open the *standard-workbench*.

. Create a new notebook and name it `model-retraining-pipeline`.

##TODO: Add steps to convert from python script into pipeline steps##

## Create a pipeline to train a model

. Create a `model-training-pipeline` directory in the `parasol-insurance` tenand directory.

. Create the `base` and `overlays` directories in the `model-training-pipeline` directory.

. In the `base` directory, create a `kustomization.yaml` file with the following content:

+
.kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - model-retrain-imagestream.yaml
  - model-retrain-pipeline.yaml
  - model-retrain-pipelinerun.yaml
  - model-retrain-rbac.yaml
  - execute-kfp-task.yaml
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-imagestream.yaml` with the following content:

+
.model-retrain-imagestream.yaml
[source,yaml]
----
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  name: model-retrain
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-pipeline.yaml` with the following content:

+
.model-retrain-pipeline.yaml
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: model-retrain
spec:
  params:
    - default: 'https://github.com/redhat-ai-services/ai-accelerator-bootcamp.git'
      description: Repo URL
      name: GIT_URL
      type: string
    - default: 'source_code/40_pipelines'
      description: Repo URL
      name: GIT_CONTEXT
      type: string
    - default: 'train-car-rekon.py'
      name: PIPELINE_SCRIPT
      type: string
    - default: main
      name: GIT_REVISION
      type: string
    - default: 3.11-ubi9
      name: PYTHON_IMAGE
      type: string
    - default: 'image-registry.openshift-image-registry.svc:5000/parasol-insurance/dsp-example'
      name: TARGET_IMAGE
      type: string
    - default: 'https://ds-pipeline-dspa.parasol-insurance.svc.cluster.local:8443'
      name: KUBEFLOW_ENDPOINT
      type: string
  tasks:
    - name: git-clone
      params:
        - name: url
          value: $(params.GIT_URL)
        - name: revision
          value: $(params.GIT_REVISION)
        - name: gitInitImage
          value: 'registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8@sha256:868966ef9d4b54952d8a74eb83bba40eb1f52c0148994fa704efd0e3797c61c5'
      taskRef:
        kind: ClusterTask
        name: git-clone
      workspaces:
        - name: output
          workspace: source
    - name: s2i-python
      params:
        - name: VERSION
          value: $(params.PYTHON_IMAGE)
        - name: PATH_CONTEXT
          value: $(params.GIT_CONTEXT)
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
      runAfter:
        - git-clone
      taskRef:
        kind: ClusterTask
        name: s2i-python
      workspaces:
        - name: source
          workspace: source
    - name: execute-kubeflow-pipeline
      params:
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
        - name: TAG
          value: latest
        - name: SCRIPT
          value: $(params.PIPELINE_SCRIPT)
        - name: KUBEFLOW_ENDPOINT
          value: $(params.KUBEFLOW_ENDPOINT)
      runAfter:
        - s2i-python
      taskRef:
        kind: Task
        name: execute-kubeflow-pipeline
  workspaces:
    - name: source
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-pipelinerun.yaml` with the following content:

+
.model-retrain-pipelinerun.yaml
[source,yaml]
----
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: model-retrain-init
  annotations:
    argocd.argoproj.io/sync-wave: "10"
spec:
  pipelineRef:
    name: model-retrain
  taskRunTemplate:
    serviceAccountName: pipeline
  timeouts:
    pipeline: 1h0m0s
  workspaces:
  - name: source
    volumeClaimTemplate:
      metadata:
        creationTimestamp: null
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
        volumeMode: Filesystem
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-rbac.yaml` with the following content:

+
.model-retrain-rbac.yaml
[source,yaml]
----
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pipelines-dsp-access
subjects:
  - kind: ServiceAccount
    name: pipelines
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: dsp-access
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dsp-access
rules:
  - verbs:
      - get
    apiGroups:
      - ''
      - route.openshift.io
    resources:
      - routes
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/execute-kfp-task.yaml` with the following content:

+
.execute-kfp-task.yaml
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: execute-kubeflow-pipeline
spec:
  description: >-
    This task will execute a python script, triggering a kubeflow pipeline
  params:
    - name: IMAGE
      description: The image used to execute the script
      type: string
    - name: TAG
      description: The tag for the image
      type: string
      default: "latest"
    - name: SCRIPT
      description: The location of the script to be executed
    - name: KUBEFLOW_ENDPOINT
      description: The endpoint URL for Kubeflow
      default: "https://ds-pipeline-dspa:8443"
  steps:
    - name: execute-python
      image: $(inputs.params.IMAGE):$(inputs.params.TAG)
      env:
        - name: KUBEFLOW_ENDPOINT
          value: $(inputs.params.KUBEFLOW_ENDPOINT)
      script: |
        python $(inputs.params.SCRIPT)
----

. In the `overlays` directory, create a `parasol-insurance-dev` directory.

. In the `parasol-insurance-dev` directory, create a `kustomization.yaml` file with the following content:

+
.kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
----

. Commit and push the changes to the Git repository.

. Wait for ArgoCD to sync the changes.

. Navigate to the OpenShift console, and validate that the `model-retrain` pipeline is available in the `parasol-insurance` namespace.

. Click on the `model-retrain` pipeline, and validate that there is a pipeline run, wait the pipeline run to complete

. Navigate to the RHOAI dashboard, and validate that the *Data Science Pipelines > Runs* has a new pipeline run with a name starting with `accident detection`.

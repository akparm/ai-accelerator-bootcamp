# Working with Pipelines

In this module we will be working with pipelines. We will be creating a pipeline that will train a model and then deploy it to a serving runtime.

## Develop a KFP pipeline to retrain a model

In the parasol-insurance repository, there is a python notebook that re-trains a model: https://github.com/rh-aiservices-bu/parasol-insurance/blob/dev/lab-materials/04/04-03-model-retraining.ipynb[04-03-model-retraining.ipynb]. We will be using this notebook to create a pipeline that will retrain the model.

. In RHOAI, open the *standard-workbench*.

. Create a new notebook and name it `model-retraining-pipeline`.

##TODO: Add steps to convert from python script into pipeline steps##

## Setup the pipelines server (DSPA).

. Create a `data-science-pipelines` directory in the `parasol-insurance` tenand directory.

. Create the `base` and `overlays` directories in the `data-science-pipelines` directory.

. Create a directory `parasol-insurance-dev` in `data-science-pipelines/overlays`.

. In the `base` directory, create a `kustomization.yaml` file with the following content:

+
.kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - dspa.yaml
----

. In the `base` directory, create a `dspa.yaml` file with the following content:

+
.dspa.yaml
[source,yaml]
----
apiVersion: datasciencepipelinesapplications.opendatahub.io/v1alpha1
kind: DataSciencePipelinesApplication
metadata:
  name: dspa
spec:
  apiServer:
    stripEOF: true
    dbConfigConMaxLifetimeSec: 120
    applyTektonCustomResource: true
    deploy: true
    enableSamplePipeline: false
    autoUpdatePipelineDefaultVersion: true
    archiveLogs: false
    terminateStatus: Cancelled
    enableOauth: true
    trackArtifacts: true
    collectMetrics: true
    injectDefaultScript: true
  database:
    disableHealthCheck: false
    mariaDB:
      deploy: true
      pipelineDBName: mlpipeline
      pvcSize: 10Gi
      username: mlpipeline
  dspVersion: v2
  objectStorage:
    disableHealthCheck: false
    enableExternalRoute: false
    externalStorage:
      basePath: ''
      bucket: pipelines
      host: 'minio.object-datastore.svc.cluster.local:9000'
      port: ''
      region: us-east-1
      s3CredentialsSecret:
        accessKey: AWS_ACCESS_KEY_ID
        secretKey: AWS_SECRET_ACCESS_KEY
        secretName: accident-model-data-conn
      scheme: http
  persistenceAgent:
    deploy: true
    numWorkers: 2
  scheduledWorkflow:
    cronScheduleTimezone: UTC
    deploy: true
----

+
[TIP]
====
Your Data Science Cluster requires of a dspa, so that you can create pipelines.

Before creating the dspa object, your Data Science Project shows a button to *Configure pipeline server*:

image::images/41_working_with_pipelines/01-configure-pipeline-server.png[Configure pipeline server]

Instead of creating the pipeline server configuration using the RHOAI method that you have seen in previous enablemens, in these steps you are using the GitOps method to create the pipeline server configuration.
====

. In the `overlays/parasol-insurance-dev/` directory, create a `kustomization.yaml` file with the following content:

+
.kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
----

. Commit and push the changes to the Git repository.

. Wait for ArgoCD to sync the changes.

. Validate that the `parasol-insurance` Data Science project allows to import pipelines under the pipelines tab.

+
[TIP]
====
Validate your code against https://github.com/redhat-ai-services/ai-accelerator-qa/pull/new/41_working_with_pipelines_dspa[Branch for DSPA configuratons]
====

## Create a pipeline to train a model

. Create a `model-training-pipeline` directory in the `parasol-insurance` tenand directory.

. Create the `base` and `overlays` directories in the `model-training-pipeline` directory.

. In the `base` directory, create a `kustomization.yaml` file with the following content:

+
.tenants/parasol-insurance/model-training-pipeline/base/kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: parasol-insurance

resources:
  - model-retrain-imagestream.yaml
  - model-retrain-pipeline.yaml
  - model-retrain-pipelinerun.yaml
  - model-retrain-rbac.yaml
  - execute-kfp-task.yaml
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-imagestream.yaml` with the following content:

+
.tenants/parasol-insurance/model-training-pipeline/base/model-retrain-imagestream.yaml
[source,yaml]
----
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  name: model-retrain
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-pipeline.yaml` with the following content:

+
.tenants/parasol-insurance/model-training-pipeline/base/model-retrain-pipeline.yaml
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: model-retrain
spec:
  params:
    - default: 'https://github.com/redhat-ai-services/ai-accelerator-bootcamp.git'
      description: Repo URL
      name: GIT_URL
      type: string
    - default: 'source_code/40_pipelines'
      description: Repo URL
      name: GIT_CONTEXT
      type: string
    - default: 'train-car-rekon.py'
      name: PIPELINE_SCRIPT
      type: string
    - default: main
      name: GIT_REVISION
      type: string
    - default: 3.11-ubi9
      name: PYTHON_IMAGE
      type: string
    - default: 'image-registry.openshift-image-registry.svc:5000/parasol-insurance/dsp-example'
      name: TARGET_IMAGE
      type: string
    - default: 'https://ds-pipeline-dspa.parasol-insurance.svc.cluster.local:8443'
      name: KUBEFLOW_ENDPOINT
      type: string
  tasks:
    - name: git-clone
      params:
        - name: url
          value: $(params.GIT_URL)
        - name: revision
          value: $(params.GIT_REVISION)
        - name: gitInitImage
          value: 'registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8@sha256:868966ef9d4b54952d8a74eb83bba40eb1f52c0148994fa704efd0e3797c61c5'
      taskRef:
        kind: ClusterTask
        name: git-clone
      workspaces:
        - name: output
          workspace: source
    - name: s2i-python
      params:
        - name: VERSION
          value: $(params.PYTHON_IMAGE)
        - name: PATH_CONTEXT
          value: $(params.GIT_CONTEXT)
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
      runAfter:
        - git-clone
      taskRef:
        kind: ClusterTask
        name: s2i-python
      workspaces:
        - name: source
          workspace: source
    - name: execute-kubeflow-pipeline
      params:
        - name: IMAGE
          value: $(params.TARGET_IMAGE)
        - name: TAG
          value: latest
        - name: SCRIPT
          value: $(params.PIPELINE_SCRIPT)
        - name: KUBEFLOW_ENDPOINT
          value: $(params.KUBEFLOW_ENDPOINT)
      runAfter:
        - s2i-python
      taskRef:
        kind: Task
        name: execute-kubeflow-pipeline
  workspaces:
    - name: source
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-pipelinerun.yaml` with the following content:

+
.tenants/parasol-insurance/model-training-pipeline/base/model-retrain-pipelinerun.yaml
[source,yaml]
----
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: model-retrain-init
  annotations:
    argocd.argoproj.io/sync-wave: "10"
spec:
  pipelineRef:
    name: model-retrain
  taskRunTemplate:
    serviceAccountName: pipeline
  timeouts:
    pipeline: 1h0m0s
  workspaces:
  - name: source
    volumeClaimTemplate:
      metadata:
        creationTimestamp: null
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
        volumeMode: Filesystem
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/model-retrain-rbac.yaml` with the following content:

+
.tenants/parasol-insurance/model-training-pipeline/base/model-retrain-rbac.yaml
[source,yaml]
----
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pipelines-dsp-access
subjects:
  - kind: ServiceAccount
    name: pipelines
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: dsp-access
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dsp-access
rules:
  - verbs:
      - get
    apiGroups:
      - ''
      - route.openshift.io
    resources:
      - routes
----

. Create file `tenants/parasol-insurance/model-training-pipeline/base/execute-kfp-task.yaml` with the following content:

+
.tenants/parasol-insurance/model-training-pipeline/base/execute-kfp-task.yaml
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: execute-kubeflow-pipeline
spec:
  description: >-
    This task will execute a python script, triggering a kubeflow pipeline
  params:
    - name: IMAGE
      description: The image used to execute the script
      type: string
    - name: TAG
      description: The tag for the image
      type: string
      default: "latest"
    - name: SCRIPT
      description: The location of the script to be executed
    - name: KUBEFLOW_ENDPOINT
      description: The endpoint URL for Kubeflow
      default: "https://ds-pipeline-dspa:8443"
  steps:
    - name: execute-python
      image: $(inputs.params.IMAGE):$(inputs.params.TAG)
      env:
        - name: KUBEFLOW_ENDPOINT
          value: $(inputs.params.KUBEFLOW_ENDPOINT)
      script: |
        python $(inputs.params.SCRIPT)
----

. In the `overlays` directory, create a `parasol-insurance-dev` directory.

. In the `parasol-insurance-dev` directory, create a `kustomization.yaml` file with the following content:

+
.tenants/parasol-insurance/model-training-pipeline/overlays/parasol-insurance-dev/kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
----

. Commit and push the changes to the Git repository.

. Wait for ArgoCD to sync the changes.

. Navigate to the OpenShift console, and validate that the `model-retrain` pipeline is available in the `parasol-insurance` namespace.

. Click on the `model-retrain` pipeline, and validate that there is a pipeline run, wait the pipeline run to complete

. Navigate to the RHOAI dashboard, and validate that the *Data Science Pipelines > Runs* has a new pipeline run with a name starting with `accident detection`.

+
[TIP]
====
Validate your code against https://github.com/redhat-ai-services/ai-accelerator-qa/pull/new/41_working_with_pipelines_model_retrain[Branch for model_retrain pipeline config]
====
